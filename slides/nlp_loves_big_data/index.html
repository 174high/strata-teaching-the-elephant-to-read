<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>NLP of Big Data using NLTK and Hadoop</title>

    <meta name="description" content="Utilize Hadoop streaming to pipe big data analytics into NLTK.">
    <meta name="author" content="Benjamin Bengfort">
    <meta name="keywords" content="NLP, NLTK, Hadoop, Big Data, Hadoop Streaming, Python, Java">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" type="text/css" href="../css/reveal.min.css" />
    <link rel="stylesheet" type="text/css" href="../css/theme/default.css" />
    <link rel="stylesheet" type="text/css" href="../lib/css/zenburn.css" />

    <link rel="stylesheet" href="../css/print/pdf.css" media="print" type="text/css">

    <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
    <![endif]-->

</head>
<body>

    <div class="reveal">
        <div class="slides">

            <section>
                <h1>NLP and Big Data</h1>
                <h2>Using NLTK and Hadoop</h2>

            </section>

            <section>
                <section>
                    <h2>NLP needs Big Data</h2>

                    <blockquote>
                    <p><small>The science that has been developed around the facts of language passed through three stages before finding its true and unique object. First something called "grammar" was studied. This study, initiated by the Greeks and continued mainly by the French, was based on logic. It lacked a scientific approach and was detached from language itself. Its only aim was to give rules for distinguishing between correct and incorrect forms; it was a normative discipline, far removed from actual observation, and its scope was limited.</small></p>
                    </blockquote>
                <p><small>-- Ferdinand de Saussure</small></p>
                </section>
                <section>
                    <h2>NLP needs Big Data</h2>
                    <p>Using Hadoop with NLTK</p>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Computational Lingusitics methodologies are stochastic</li>
                        <li class="fragment" data-fragment-index="1">Examples are easier to create than rules</li>
                        <li class="fragment" data-fragment-index="2">Rules and Logic miss frequency and language dynamics</li>
                        <li class="fragment" data-fragment-index="3">Humans use lots of data for the same task- It's AI!</li>
                        <li class="fragment" data-fragment-index="4">More data is better - relevance is in the long tail</li>
                        <li class="fragment" data-fragment-index="5">If you don't have enough data - hire a knowledge engineer</li>
                    </ul>
                </section>

                <section>
                    <h2>Big Data will need NLP</h2>
                    <a href="img/GoodReads Reviews.png" class="image"><img src="img/GoodReads Reviews.png" style="max-height:350px;" /></a>
                    <a href="img/Tweets.png" class="image"><img src="img/Tweets.png" style="max-height:350px;" /></a>
                    <a href="img/email.png" class="image"><img src="img/email.png" style="max-height:350px;" /></a>
                </section>

                <section>
                    <h2>Big Data will need NLP</h2>
                    <p>Using NLTK with Hadoop</p>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Hadoop is great at massive amounts of text data</li>
                        <li class="fragment" data-fragment-index="1">However, current methods aren't really NLP</li>
                        <li class="fragment" data-fragment-index="2">Indexing, Co-Occurrence, even N-Gram Modeling is search</li>
                        <li class="fragment" data-fragment-index="3">We haven't exhausted frequency analysis yet</li>
                        <li class="fragment" data-fragment-index="4">But when we do, we're going to want semantic analyses</li>
                    </ul>
                </section>

            </section>

            <section>
                <section>
                    <h2>Domain Knowledge is <em>IMPORTANT</em></h2>
                    <a href="img/brief.jpg" class="image"><img src="img/brief.jpg"></a>
                    <a href="img/tweet.jpg" class="image"><img src="img/tweet.jpg"></a>
                    <a href="img/poem.jpg" class="image"><img src="img/poem.jpg"></a>

                    <ul>
                        <li class="fragment" data-fragment-index="0">Stochastic methods are not universal</li>
                        <li class="fragment" data-fragment-index="1">Domain specific training sets and knowledge required</li>
                    </ul>
                </section>

                <section>
                    <h2>Capitol Words</h2>
                    <a href="img/open_government.png" class="image"><img src="img/open_government.png" /></a>
                    <p><small>From <a href="http://sunlightfoundation.com/blog/tag/capitol-words/">How Congress Talks about Sunshine Week</a></small></p>
                </section>
            </section>
            <section>
                <h2>The Foo of Big Data</h2>
                <p>Given a large data set and domain specific knowledge:</p>
                <ol>
                    <li class="fragment" data-fragment-index="0">Form hypothesis (a data product)</li>
                    <li class="fragment" data-fragment-index="1">Mix in NLP techniques and machine learning tools</li>
                    <li class="fragment" data-fragment-index="2">Perform computation and test hypothesis</li>
                    <li class="fragment" data-fragment-index="3">Add to data set and domain knowledge</li>
                    <li class="fragment" data-fragment-index="4">Repeat</li>
                </ol>
                <br />
                <br />
                <p class="fragment" data-fragment-index="5">We have a wealth of data and can iterate rapidly!</p>
            </section>

            <section>

                <section>
                        <h2>Why NLTK?</h2>
                        <a href="img/python-zen.png" class="image"><img src="img/python-zen.png" /></a>
                        <ul>
                            <li class="fragment" data-fragment-index="0">It's not Stanford</li>
                            <li class="fragment" data-fragment-index="1">It's Open Source (the price is right)</li>
                            <li class="fragment" data-fragment-index="2">The blessing (and curse) of choice</li>
                            <li class="fragment" data-fragment-index="3"><em>It lets you use your domain knowledge (it forces you to)</em></li>
                        </ul>
                </section>

                <section data-markdown>
                    <script type="text/template">
                        ## Why NLTK? ##

                        [![Batteries Included][batteries_included.jpg]][batteries_included.jpg]

                        * Batteries included
                        * Fast prototyping and iteration
                        * Strong community
                        * out of the box NLP

                        [batteries_included.jpg]: img/batteries.jpg

                    </script>
                </section>

            </section>
            <section>
                <section>
                        <h2>Why Hadoop?</h2>
                        <a href="img/hadoop-blue.jpg" class="image"><img src="img/hadoop-blue.jpg" /></a>
                        <ul>
                            <li class="fragment" data-fragment-index="0">Silly question at a Big Data talk?</li>
                            <li class="fragment" data-fragment-index="1">Who doesn't have a distribution? Intel has one!</li>
                            <li class="fragment" data-fragment-index="2">NLP is embarrassingly parallel, perfect for Map Reduce.</li>
                            <li class="fragment" data-fragment-index="3">You've got a cluster in your closet.</li>
                        </ul>
                </section>

                <section>
                    <h3>Does Hadoop really do native tokenization?</h3>
                    <a href="img/babar.jpg" class="image"><img src="img/babar.jpg" style="max-height:200px;" /></a>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Word count is your first Hadoop program</li>
                        <li class="fragment" data-fragment-index="1">(sometimes also called segmentation or chunking)</li>
                        <li class="fragment" data-fragment-index="2">Not as simple as splitting on punctuation and whitespace</li>
                        <li class="fragment" data-fragment-index="4">Different NLP tasks require different kinds of tokenization</li>
                    </ul>
                    <br /><br />
                    <blockquote class="fragment" data-fragment-index="3">
                        <p><small>You're not going to the U.S.A. in that super-zeppelin, Dr. Stoddard?</small></p>
                    </blockquote>
                </section>

                <section>
                    <h3>Preprocessing Unstructured Text</h3>
                    <a href="img/king-babar.jpg" class="image"><img src="img/king-babar.jpg" style="max-height:200px;" /></a>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Hadoop stores output as it's own file</li>
                        <li class="fragment" data-fragment-index="1">Map/Reduce jobs are now essentially built-in preprocessors</li>
                        <li class="fragment" data-fragment-index="2">Last-mile computation can be done in 100GB of Memory</li>
                        <li class="fragment" data-fragment-index="3">Hadoop is best for a series of jobs that transform data to something machine tractable</li>
                        <li class="fragment" data-fragment-index="4">In NLP this means: text → tokenized → tagged → parsed → Treebank</li>
                    </ul>
                </section>



            </section>

            <section>
                <section>
                    <a href="img/python-vs-java.jpg" class="image"><img src="img/python-vs-java.jpg" /></a>
                    <p>Hadoop is Java and NLTK is Python, how to make them play?</p>
                    <br /><br />
                    <p><small>Comic Attribution: <a href="http://askrahul.com/blog/from-funny-moments-to-emails-to-sms/python-vs-java/">askrahul.com</a></small></p>
                </section>
                <section>
                    <h2>Now We Start Typing</h2>
                    <ul>
                        <li>Intro to Hadoop Streaming with Python</li>
                        <li>An NLP token count with Dumbo</li>
                    </ul>
                </section>
            </section>
            <section>
                <section>
                    <h2>Hadoop Streaming</h2>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Supply any executable to Hadoop as the mapper or reducer</li>
                        <li class="fragment" data-fragment-index="1">Key Value pairs read from <code>stdin</code> and pushed to <code>stdout</code></li>
                        <li class="fragment" data-fragment-index="2">All Hadoopy-ness still exists, only the mapper and reducer get to be replaced</li>
                    </ul>
                </section>

                <section>
                    <h3>mapper.py</h3>
                    <pre><code>
    import sys

    class Mapper(object):

        def __init__(self, infile=sys.stdin, separator='\t'):
            self.infile = infile
            self.sep    = separator

        def emit(self, key, value):
            sys.stdout.write("%s%s%s\n" % (key, self.sep, value))

        def map(self):
            for line in self:
                for word in line.split():
                    self.emit(word, 1)

        def __iter__(self):
            for line in self.infile:
                yield line

    if __name__ == "__main__":
        mapper = Mapper()
        mapper.map()</code></pre>
                </section>

                <section>
                    <h3>reducer.py</h3>
                    <pre><code>
    import sys
    from itertools import groupby
    from operator import itemgetter

    class Reducer(object):

        def __init__(self, infile=sys.stdin, separator="\t"):
            self.infile = infile
            self.sep    = separator

        def emit(self, key, value):
            sys.stdout.write("%s%s%s\n" % (key, self.sep, value))

        def reduce(self):
            for current, group in groupby(self, itemgetter(0)):
                try:
                    total = sum(int(count) for current, count in group)
                    self.emit(current, total)
                except ValueError:
                    pass

        def __iter__(self):
            for line in self.infile:
                yield line.rstrip().split(self.sep, 1)

    if __name__ == "__main__":
        reducer = Reducer()
        reducer.reduce()</pre></code>
                </section>

                <section>
                    <h3>Running the Job</h3>
                    <pre><code>
    hduser@ubuntu:/usr/local/hadoop$ bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar \
        -file /home/hduser/mapper.py    -mapper /home/hduser/mapper.py \
        -file /home/hduser/reducer.py   -reducer /home/hduser/reducer.py \
        -input /user/hduser/gutenberg/* -output /user/hduser/gutenberg-output</code></pre>
                </section>
            </section>

            <section>
                <section data-markdown>
                    <script type="text/template">
                        ## MapReduce With Dumbo ##

                        * Serialization with TypedBytes
                        * Unix Pipes for development
                        * Wraps Hadoop Streaming Jar
                        * Manages jobs, starters, and iterationl
                        * Allows you to focus on MapReduce

                    </script>
                </section>
                <section>
                    <h3>token_count.py</h3>
                    <pre><code>
    import nltk

    from nltk.stem import WordNetLemmatizer
    from nltk.tokenize import wordpunct_tokenize

    class Mapper(object):

        def __init__(self):
            if 'stopwords' in self.params:
                with open(self.params['stopwords'], 'r') as excludes:
                    self._stopwords = set(line.strip() for line in excludes)
            else:
                self._stopwords = None

            self.lemmatizer = WordNetLemmatizer()

        def __call__(self, key, value):
            for word in self.tokenize(value):
                if not word in self.stopwords:
                    yield word, 1

        def normalize(self, word):
            word = word.lower()
            return self.lemmatizer.lemmatize(word)

        def tokenize(self, sentence):
            for word in wordpunct_tokenize(sentence):
                yield self.normalize(word)

        @property
        def stopwords(self):
            if not self._stopwords:
                self._stopwords = nltk.corpus.stopwords.words('english')
            return self._stopwords

    def reducer(key, values):
        yield key, sum(values)

    def runner(job):
        job.additer(Mapper, reducer, reducer)

    def starter(prog):
        excludes = prog.delopt("stopwords")
        if excludes: prog.addopt("param", "stopwords="+excludes)

    if __name__ == "__main__":
        import dumbo
        dumbo.main(runner, starter)</code></pre>
                </section>

                <section>
                    <h3>Running the job</h3>
                    <pre><code>
    hduser@ubuntu:~$ dumbo start token_count.py \
        -input /user/hduser/gutenberg
        -output /user/hduser/gutenberg-output
        -hadoop $HADOOP_BIN
        -hadooplib $HADOOP_CLASSPATH</code></pre>
                </section>
            </section>

            <section>
                <section>
                    <h2>Important Notes</h2>
                    <ul>
                        <li class="fragment" data-fragment-index="0">An Interpreter is loaded for every job (no multiprocessing)</li>
                        <li class="fragment" data-fragment-index="1">NLTK data loading only happens <em>ONCE</em>!</li>
                        <li class="fragment" data-fragment-index="2">Use generators to save on memory</li>
                        <li class="fragment" data-fragment-index="3">Other Libraries exist for quickly creating tools.</li>
                    </ul>
                </section>
                <section>
                    <h3>Pro Tips</h3>
                    <a href="img/elephant-ninja-funny.jpg" class="image"><img src="img/elephant-ninja-funny.jpg" style="max-height:200px;" /></a>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Reusable tasks for generating domain-specific knowledge</li>
                        <li class="fragment" data-fragment-index="1">NLTK Trainer loads from Pickled Data</li>
                        <li class="fragment" data-fragment-index="2">We generated tag data sets, lexicons, PCFGs</li>
                        <li class="fragment" data-fragment-index="3">10-fold training/test/validation on your corpus</li>
                    </ul>
                </section>
            </section>

            <section>
                <section>
                    <h2>Next we need to handle Big Data</h2>
                    <a href = "../index.html">Back to Main Page</a>
                </section>
            </section>


        </div>
    </div>

    <!-- JavaScript at the bottom of the page for faster page loading. -->
    <script type="text/javascript" src="../lib/js/head.min.js"></script>
    <script type="text/javascript" src="../js/reveal.min.js"></script>

        <script type="text/javascript">
        Reveal.initialize({
            controls: true,
            progress: true,
            history:  false,
            keyboard: true,
            touch:    false,
            overview: true,
            center:   true,
            loop:     false,
            transition: 'default',
            transitionSpeed: 'default',
            backgroundTransition: 'default',
            dependencies: [
                // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
                { src: '../lib/js/classList.js', condition: function() { return !document.body.classList; } },

                // Interpret Markdown in <section> elements
                { src: '../plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

                // Syntax highlight for <code> elements
                { src: '../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

                // Zoom in and out with Alt+click
                { src: '../plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
            ]
        });
    </script>
    <!--
    <script>

        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            center: true,
            dependencies: [
					{ src: 'js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'js/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
				]

        });

    </script>
    -->

</body>
</html>
