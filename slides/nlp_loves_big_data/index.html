<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>NLP of Big Data using NLTK and Hadoop</title>

    <meta name="description" content="Utilize Hadoop streaming to pipe big data analytics into NLTK.">
    <meta name="author" content="Benjamin Bengfort">
    <meta name="keywords" content="NLP, NLTK, Hadoop, Big Data, Hadoop Streaming, Python, Java">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" type="text/css" href="../css/reveal.min.css" />
    <link rel="stylesheet" type="text/css" href="../css/theme/default.css" />
    <link rel="stylesheet" type="text/css" href="../lib/css/zenburn.css" />    

    <link rel="stylesheet" href="../css/print/pdf.css" media="print" type="text/css">

    <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
    <![endif]-->

</head>
<body>
    
    <div class="reveal">
        <div class="slides">

            <section>
                <h1>NLP and Big Data</h1>
                <h2>Using NLTK and Hadoop</h2>

            </section>

            <section>
                <section>
                    <h2>NLP needs Big Data</h2>

                    <blockquote>
                    <p><small>The science that has been developed around the facts of language passed through three stages before finding its true and unique object. First something called "grammar" was studied. This study, initiated by the Greeks and continued mainly by the French, was based on logic. It lacked a scientific approach and was detached from language itself. Its only aim was to give rules for distinguishing between correct and incorrect forms; it was a normative discipline, far removed from actual observation, and its scope was limited.</small></p>
                    </blockquote>
                <p><small>-- Ferdinand de Saussure</small></p>
                </section>
                <section>
                    <h2>NLP needs Big Data</h2>
                    <p>Using Hadoop with NLTK</p>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Computational Lingusitics methodologies are stochastic</li>
                        <li class="fragment" data-fragment-index="1">Examples are easier to create than rules</li>
                        <li class="fragment" data-fragment-index="2">Rules and Logic miss frequency and language dynamics</li>
                        <li class="fragment" data-fragment-index="3">Humans use lots of data for the same task- It's AI!</li>
                        <li class="fragment" data-fragment-index="4">More data is better - relevance is in the long tail</li>
                        <li class="fragment" data-fragment-index="5">If you don't have enough data - hire a knowledge engineer</li>
                    </ul>
                </section>

                <section>
                    <h2>Big Data will need NLP</h2>
                    <a href="img/GoodReads Reviews.png" class="image"><img src="img/GoodReads Reviews.png" style="max-height:350px;" /></a>
                    <a href="img/Tweets.png" class="image"><img src="img/Tweets.png" style="max-height:350px;" /></a>
                    <a href="img/email.png" class="image"><img src="img/email.png" style="max-height:350px;" /></a>
                </section>

                <section>
                    <h2>Big Data will need NLP</h2>
                    <p>Using NLTK with Hadoop</p>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Hadoop is great at massive amounts of text data</li>
                        <li class="fragment" data-fragment-index="1">However, current methods aren't really NLP</li>
                        <li class="fragment" data-fragment-index="2">Indexing, Co-Occurrence, even N-Gram Modeling is search</li>
                        <li class="fragment" data-fragment-index="3">We haven't exhausted frequency analysis yet</li>
                        <li class="fragment" data-fragment-index="4">But when we do, we're going to want semantic analyses</li>
                    </ul>
                </section>

            </section>

            <section>
                <section>
                    <h2>Domain Knowledge is <em>IMPORTANT</em></h2>
                    <a href="img/brief.jpg" class="image"><img src="img/brief.jpg"></a>
                    <a href="img/tweet.jpg" class="image"><img src="img/tweet.jpg"></a>
                    <a href="img/poem.jpg" class="image"><img src="img/poem.jpg"></a>

                    <ul>
                        <li class="fragment" data-fragment-index="0">Stochastic methods are not universal</li>
                        <li class="fragment" data-fragment-index="1">Domain specific training sets and knowledge required</li>
                    </ul>
                </section>
               
                <section>
                    <h2>Capitol Words</h2>
                    <a href="img/open_government.png" class="image"><img src="img/open_government.png" /></a>
                    <p><small>From <a href="http://sunlightfoundation.com/blog/tag/capitol-words/">How Congress Talks about Sunshine Week</a></small></p>
                </section>

                <section>
                    <h2>The Foo of Big Data</h2>
                    <p>Given a large data set and domain specific knowledge:</p>
                    <ol>
                        <li class="fragment" data-fragment-index="0">Form hypothesis (a data product)</li>
                        <li class="fragment" data-fragment-index="1">Mix in NLP techniques and machine learning tools</li>
                        <li class="fragment" data-fragment-index="2">Perform computation and test hypothesis</li>
                        <li class="fragment" data-fragment-index="3">Add to data set and domain knowledge</li>
                        <li class="fragment" data-fragment-index="4">Repeat</li>
                    </ol>
                    <br />
                    <br />
                    <p class="fragment" data-fragment-index="5">We have a wealth of data and can iterate rapidly!</p>
                </section>
                
            </section>

            <section>

                <section>
                        <h2>Why NLTK?</h2>
                        <a href="img/python-zen.png" class="image"><img src="img/python-zen.png" /></a>
                        <ul>
                            <li class="fragment" data-fragment-index="0">It's not Stanford</li>
                            <li class="fragment" data-fragment-index="1">It's Open Source (the price is right)</li>
                            <li class="fragment" data-fragment-index="2">The blessing (and curse) of choice</li>
                            <li class="fragment" data-fragment-index="3"><em>It lets you use your domain knowledge (it forces you to)</em></li>
                            <li class="fragment" data-fragment-index="4">Out of the box NLP</li>
                        </ul>
                </section>
                
                <section>
                        <h2>Why Hadoop?</h2>
                        <a href="img/hadoop-blue.jpg" class="image"><img src="img/hadoop-blue.jpg" /></a>                        
                        <ul>
                            <li class="fragment" data-fragment-index="0">Silly question at a Big Data talk?</li>
                            <li class="fragment" data-fragment-index="1">Who doesn't have a distribution? Intel has one!</li>
                            <li class="fragment" data-fragment-index="2">NLP is embarrassingly parallel, perfect for Map Reduce.</li>
                            <li class="fragment" data-fragment-index="3">You've got a cluster in your closet.</li>
                        </ul>                
                </section>
                
                <section>
                        <a href="img/cluster.png" class="image"><img src="img/cluster.png" /></a>
                        <p><small>The beginning of a startup's seven node cluster, total cost less than $1500</small></p>
                </section>
            </section>

            <section>
                <section>   
                    <a href="img/python-vs-java.jpg" class="image"><img src="img/python-vs-java.jpg" /></a>
                    <p>Hadoop is Java and NLTK is Python, how to make them play?</p>
                    <br /><br />
                    <p><small>Comic Attribution: <a href="http://askrahul.com/blog/from-funny-moments-to-emails-to-sms/python-vs-java/">askrahul.com</a></small></p>
                </section>
                <section>
                    <h2>Hadoop Streaming</h2>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Supply any executable to Hadoop as the mapper or reducer</li>
                        <li class="fragment" data-fragment-index="1">Key Value pairs read from <code>stdin</code> and pushed to <code>stdout</code></li>
                        <li class="fragment" data-fragment-index="2">All Hadoopy-ness still exists, only the mapper and reducer get to be replaced</li>
                    </ul>
                </section>

                <section>
                    <h3>mapper.py</h3>
                    <pre><code>
    #!/usr/bin/env python

    import sys
    from nltk.tokenize import wordpunct_tokenize

    def read_input(file):
        for line in file:
            # split the line into tokens
            yield wordpunct_tokenize(line)

    def main(separator='\t'):
        # input comes from STDIN (standard input)
        data = read_input(sys.stdin)
        for tokens in data:
            # write the results to STDOUT (standard output);
            # what we output here will be the input for the
            # Reduce step, i.e. the input for reducer.py
            #
            # tab-delimited; the trivial token count is 1
            for token in tokens:
                print '%s%s%d' % (word, separator, 1)

    if __name__ == "__main__":
        main()</code></pre>
                </section>

                <section>
                    <h3>reducer.py</h3>
                    <pre><code>
    #!/usr/bin/env python

    from itertools import groupby
    from operator import itemgetter
    import sys

    def read_mapper_output(file, separator='\t'):
        for line in file:
            yield line.rstrip().split(separator, 1)

    def main(separator='\t'):
        # input comes from STDIN (standard input)
        data = read_mapper_output(sys.stdin, separator=separator)
        # groupby groups multiple word-count pairs by word,
        # and creates an iterator that returns consecutive keys and their group:
        #   current_word - string containing a word (the key)
        #   group - iterator yielding all ["&lt;current_word&gt;", "&lt;count&gt;"] items
        for current_word, group in groupby(data, itemgetter(0)):
            try:
                total_count = sum(int(count) for current_word, count in group)
                print "%s%s%d" % (current_word, separator, total_count)
            except ValueError:
                # count was not a number, so silently discard this item
                pass

    if __name__ == "__main__":
        main()</pre></code>
                </section>

                <section>
                    <h3>Running the Job</h3>
                    <pre><code>
    hduser@ubuntu:/usr/local/hadoop$ bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar \
        -file /home/hduser/mapper.py    -mapper /home/hduser/mapper.py \
        -file /home/hduser/reducer.py   -reducer /home/hduser/reducer.py \
        -input /user/hduser/gutenberg/* -output /user/hduser/gutenberg-output</code></pre>
                </section>

                <section>
                    <h2>Important Notes</h2>
                    <ul>
                        <li class="fragment" data-fragment-index="0">An Interpreter is loaded for every job (no multiprocessing)</li>
                        <li class="fragment" data-fragment-index="1">NLTK data loading only happens <em>ONCE</em>!</li>
                        <li class="fragment" data-fragment-index="2">Use generators to save on memory</li>
                        <li class="fragment" data-fragment-index="3">Libraries exist for quickly creating tools (Dumbo)</li>
                    </ul>
                </section>
            </section>

            <section>
                <a href="img/deep-end.jpg" class="image"><img src="img/deep-end.jpg" /></a>
                <p>On to NLP/Big Data Nuts and Bolts</p>
            </section>

            <section>

                <section>
                    <h3>Does Hadoop really do native tokenization?</h3>
                    <a href="img/babar.jpg" class="image"><img src="img/babar.jpg" style="max-height:200px;" /></a>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Word count is your first Hadoop program</li>
                        <li class="fragment" data-fragment-index="1">(sometimes also called segmentation or chunking)</li>
                        <li class="fragment" data-fragment-index="2">Not as simple as splitting on punctuation and whitespace</li>
                        <li class="fragment" data-fragment-index="4">Different NLP tasks require different kinds of tokenization</li>
                    </ul>
                    <br /><br />
                    <blockquote class="fragment" data-fragment-index="3">
                        <p><small>You're not going to the U.S.A. in that super-zeppelin, Dr. Stoddard?</small></p>
                    </blockquote>
                </section>

                <section>
                    <h3>Preprocessing Unstructured Text</h3>
                    <a href="img/king-babar.jpg" class="image"><img src="img/king-babar.jpg" style="max-height:200px;" /></a>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Hadoop stores output as it's own file</li>
                        <li class="fragment" data-fragment-index="1">Map/Reduce jobs are now essentially built-in preprocessors</li>
                        <li class="fragment" data-fragment-index="2">Last-mile computation can be done in 100GB of Memory</li>
                        <li class="fragment" data-fragment-index="3">Hadoop is best for a series of jobs that transform data to something machine tractable</li>
                        <li class="fragment" data-fragment-index="4">In NLP this means: text → tokenized → tagged → parsed → Treebank</li>
                    </ul>
                </section>

            </section>
            
            <section>
                <section>
                    <h2>The NLTK Preprocessing Toolkit</h2>
                    <a href="img/space-tools.jpg" class="image"><img src="img/space-tools.jpg" style="max-height:300px;" /></a>
                    <p><small>Apparently this tool is used in space!</small></p>
                </section>
                <section>
                    <p>Breaking the strings with NLTK:</p>
                    <p><small>Tokenization</small></p>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Punkt Sentence Tokenizer (segmenter)</li>
                        <li class="fragment" data-fragment-index="1">Punkt Word Tokenizer (language vars)</li>
                        <li class="fragment" data-fragment-index="2">Punkt uses unsupervised model for abbreviation words, collocations, sentence start- must be trained</li>
                        <li class="fragment" data-fragment-index="3">Regexp (rule based tokenizer)</li>
                        <li class="fragment" data-fragment-index="4">Text Tiling - detects subtopic shifts based on lexical co-occurence</li>
                        <li class="fragment" data-fragment-index="5">Treebank tokenizer</li>
                    </ul>
                </section>

                <section>
                    <p>Understanding Syntactic Class</p>
                    <p><small>Tagging</small></p>
                    <ul>
                        <li class="fragment" data-fragment-index="0">From simple tokenization to a better understanding of the syntactic role of the tokens.</li>
                        <li class="fragment" data-fragment-index="1">Tagging is another stochastic process.</li>
                        <li class="fragment" data-fragment-index="2">Lexical decision making - most likely tag for a word.</li>
                        <li class="fragment" data-fragment-index="3">Trigram Tagger - chooses tags based on preceeding two tokens' tags</li>
                        <li class="fragment" data-fragment-index="4">BrillTagger - transformational rule-based tagger</li>
                        <li class="fragment" data-fragment-index="5">Hidden Markov Model Tagger - a generative model</li>
                    </ul>
                </section>

                <section>
                    <p>Discovering Structure</p>
                    <p><small>Parsing</small></p>
                    <a href="img/parse-tree.png" class="image"><img src="img/parse-tree.png" style="max-height:200px;" /></a>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Parsing requires knowledge: a context free grammar</li>
                        <li class="fragment" data-fragment-index="1">Probabalistic CFGs add ambiguity detection and correction</li>
                        <li class="fragment" data-fragment-index="2">Dynamic Programming Algorithms</li>
                        <li class="fragment" data-fragment-index="3">Chart Parser: Adds edges to a chart (Earley and Feature)</li>
                        <li class="fragment" data-fragment-index="4">Viterbi Parser: "Most Likely Constituent Table"</li>
                    </ul>
                </section>

                <section>
                    <p>The final result of preprocessing</p>
                    <p><small>Treebanks</small></p>
                    <a href="img/treebank.jpg" class="image"><img src="img/treebank.jpg" style="max-height:300px;" /></a>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Treebanks can be human-annotated or -reviewed to create better training sets</li>
                        <li class="fragment" data-fragment-index="1">Annotated treebanks can be the basis for <em>all</em> previous stochastic mechanisms</li>
                    </ul>
                </section>

            </section>
            
            <section>
                <section>
                    <h3>Hadoop and Preprocessing</h3>
                    <a href="img/king-babar.jpg" class="image"><img src="img/king-babar.jpg" style="max-height:200px;" /></a>
                    <ul>
                        <li class="fragment" data-fragment-index="0">All the previous steps are well suited to Map Reduce</li>
                        <li class="fragment" data-fragment-index="1">NLP is embarrassingly parallel</li>
                        <li class="fragment" data-fragment-index="3">Mappers operate on preprocessed text at all levels</li>
                        <li class="fragment" data-fragment-index="4">Reducers use document ids to generate intermediary stages</li>
                    </li>
                </section>

                <section>
                    <h3>Pro Tips</h3>
                    <a href="img/elephant-ninja-funny.jpg" class="image"><img src="img/elephant-ninja-funny.jpg" style="max-height:200px;" /></a>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Reusable tasks for generating domain-specific knowledge</li>
                        <li class="fragment" data-fragment-index="1">NLTK Trainer loads from Pickled Data</li>
                        <li class="fragment" data-fragment-index="2">We generated tag data sets, lexicons, PCFGs</li>
                        <li class="fragment" data-fragment-index="3">10-fold training/test/validation on your corpus</li>
                        <li class="fragment" data-fragment-index="4">Did I mention NLTK has built in NER? <code>nltk.ne_chunk</code></li>
                    </ul>
                </section>
            </section>
            
            <!-- 
            <section>
                <h2>Quick ML in NLTK</h2>
                <p class="fragment" data-fragment-index="0">Quick Confession: I didn't do Mahout...</p>
                <ul>
                    <li class="fragment" data-fragment-index="1">nltk.classify</li>
                    <li class="fragment" data-fragment-index="2">nltk.cluster</li>
                    <li class="fragment" data-fragment-index="3">nltk-trainer for classifier</li>
                    <li class="fragment" data-fragment-index="4">Naive Bayes &amp; Max Entropy</li>
                    <li class="fragment" data-fragment-index="5">Not NLTK: Scipy & Numpy</li>
                </ul>
            </section>
            -->

            <section>
                <section>
                    <h2>Meaning (semantics)</h2>
                    <a href="img/semantic-comment.gif" class="image"><img src="img/semantic-comment.gif" style="max-height:250px"/></a>
                    <p>Syntatic guessing is not the final frontier of stochastic NLP</p>
                    <p>Create machine-tractable text meaning representations</p>
                    <br /><br />
                    <p><small>Comic from <a href="http://speedbump.com">SpeedBump.com</a></small></p>
                </section>

                <section>
                    <h2>Text Meaning Representations</h2>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Language-neutral representation of a language unit</li>
                        <li class="fragment" data-fragment-index="1">A series of connected frames representing knowledge</li>
                        <li class="fragment" data-fragment-index="2">Requires Ontologies/Taxonomies large knowledge bases</li>
                        <li class="fragment" data-fragment-index="3">Allows deep querying of text content</li>
                    </ul>
                </section>

                <section>
                    <h2>Lightweight Semantic Documents -- WIMs</h2>
                    <a href="img/wim.png" class="image"><img src="img/wim.png" style="max-height:250px"/></a>
                    <ul>
                        <li class="fragment" data-fragment-index="0">Limited, but important set of relations</li>
                        <li class="fragment" data-fragment-index="1">Lightweight processing using available knowledge</li>
                        <li class="fragment" data-fragment-index="2">Creates a traversable graph of the content</li>
                        <li class="fragment" data-fragment-index="3">Open Source!</li>
                    </ul>
                </section>
            </section>
            
            <section>
                <h2>Quick Recap</h2>
                <ul>
                    <li>NLP needs Big Data</li>
                    <li>Big Data will need NLP</li>
                    <li>Domain Knowledge + Data Set = Foo!</li>
                    <li>NLTK &amp; Hadoop good for preprocessing &amp; stochastic tasks</li>
                    <li>Hadoop may not be the best for large scale TMR graphing</li>
                </ul>
            </section>
        </div>
    </div>

    <!-- JavaScript at the bottom of the page for faster page loading. -->
    <script type="text/javascript" src="../lib/js/head.min.js"></script>
    <script type="text/javascript" src="../js/reveal.min.js"></script>
    
        <script type="text/javascript">
        Reveal.initialize({
            controls: true,
            progress: true,
            history:  false,
            keyboard: true,
            touch:    false,
            overview: true,
            center:   true,
            loop:     false,
            transition: 'default',
            transitionSpeed: 'default',
            backgroundTransition: 'default',
            dependencies: [
                // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
                { src: '../lib/js/classList.js', condition: function() { return !document.body.classList; } },

                // Interpret Markdown in <section> elements
                { src: '../plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                { src: '../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

                // Syntax highlight for <code> elements
                { src: '../plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

                // Zoom in and out with Alt+click
                { src: '../plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
            ]
        });
    </script>
    <!--
    <script>
        
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            center: true,
            dependencies: [
					{ src: 'js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'js/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
				]

        });

    </script>
    -->

</body>
</html>
